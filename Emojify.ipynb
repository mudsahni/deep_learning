{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim:\n",
    "To implement a model which takes a sentence as an input and finds the most appropriate emoji to be used along with it.\n",
    "\n",
    "_Example:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This package will help us convert text to emoji\n",
    "import emoji\n",
    "\n",
    "sentence = \"I'm tired of using tensorflow!\"\n",
    "print(f\"{sentence} {emoji.emojize(':disappointed:',use_aliases=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many emoji interfaces, you need to remember that ❤️ is the \"heart\" symbol rather than the \"love\" symbol. But using word vectors, you'll see that even if your training set explicitly relates only a few words to a particular emoji, your algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don't even appear in the training set. This allows you to build an accurate classifier mapping from sentences to emojis, even using a small training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in the data\n",
    "train = pd.read_csv('data/train_data.csv', names = [\"sentence\",\"emoji\",\"unk1\",\"unk2\"])\n",
    "test = pd.read_csv('data/tesss.csv', names = [\"sentence\",\"emoji\",\"unk1\",\"unk2\"])\n",
    "\n",
    "X_train, Y_train = train['sentence'].values, np.asarray(train['emoji'].values,dtype=int)\n",
    "X_test, Y_test = test['sentence'].values, np.asarray(test['emoji'].values, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a very tiny dataset. `X_train` contains 132 sentences and `Y_train` corresponding integers for suitable emojis. Let's build the emoji dictionary mapping, matching integer to appropriate emoji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to convert the integer to an emoji. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(X_train[i], label_to_emoji(Y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "The input of the model is a string corresponding to a sentence (e.g. \"I love you). In the code, the output will be a probability vector of shape (1,5), that you then pass in an argmax layer to extract the index of the most likely emoji output.\n",
    "\n",
    "To get our labels into a format suitable for training a softmax classifier, lets convert $Y$ from its current shape current shape $(m, 1)$ into a \"one-hot representation\" $(m, 5)$, where each row is a one-hot vector giving the label of one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert to one-hot-encoding\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How the one hot encoded vector looks like\n",
    "index = 50\n",
    "print(Y_train[index], \"is converted into one hot\", Y_oh_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "\n",
    "The first step is to convert an input sentence into the word vector representation, which then get averaged together. We will use pretrained 50-dimensional GloVe embeddings on the Wikipedia2014-Gigabit dataset. Let's start by writing a function on how to parse the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    #opening the glove file\n",
    "    with open(glove_file, \"r\") as f:\n",
    "        #creating an empty set for unique words\n",
    "        words = set()\n",
    "        #creating a dictionary to map words to their vectors\n",
    "        word_to_vec_map = {}\n",
    "        #going through the text file line by line (each line is a word and its vectors)\n",
    "        for line in f:\n",
    "            #removing any white spaces and splitting the line into individual components\n",
    "            line = line.strip().split()\n",
    "            #extracting the word\n",
    "            current_word = line[0]\n",
    "            #adding to the set\n",
    "            words.add(current_word)\n",
    "            #adding to the dictionary along with its vector representation\n",
    "            word_to_vec_map[current_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "        #starting an index counter\n",
    "        i = 1\n",
    "        #creating a dictionary to map words to an index\n",
    "        words_to_index = {}\n",
    "        #creating a dictionary to map index to the words\n",
    "        index_to_words = {}\n",
    "        #iterating through the list of unique words\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i+=1\n",
    "    \n",
    "    #returning word-index maps and the word2vec map\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, we've ended up loading:\n",
    "- word_to_index - map of 400,001 words to their valid integer indexes\n",
    "- index_to_words - dictionary mapping of indexes to their valid words in the vocabulary\n",
    "- word_to_vec_map - dictionary mapping of words to their GloVe vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a look\n",
    "word = \"cucumber\"\n",
    "index = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word2Vec Sentence Representation \n",
    "Let's write a function to extract the average of the GloVe vector map for a sentence. The function should do the following:\n",
    "1. Convert every sentene to lower case and split it to individual words.\n",
    "2. For every word, the function extracts the corresponding GloVe representation and then averages all these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Aim:\n",
    "        Function to extract the average of the GloVe vector map for a sentence.\n",
    "    Arguments:\n",
    "        sentence - [str] one example of a sentence\n",
    "        word_to_vec_map - dictionary of words mapped to their GloVe vectors\n",
    "    Returns:\n",
    "        average - average vector encoding information about the sentence, \n",
    "                  numpy array of shape (GloVe vector size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    #splitting and standardizing the sentence words\n",
    "    words = [i.lower() for i in sentence.split()]\n",
    "    \n",
    "    #size of the GloVe vector features for each word\n",
    "    m = word_to_vec_map['the'].shape[0]\n",
    "    \n",
    "    average = np.zeros((m,))\n",
    "    for w in words:\n",
    "        #we check if the word is part of the vocabulary\n",
    "        try:\n",
    "            average += word_to_vec_map[w]\n",
    "        #else we initialize a random weight vector from a normal distribution\n",
    "        except KeyError:\n",
    "            average += np.random.normal(scale=0.6,size=(m,))\n",
    "    \n",
    "    average /= len(words)\n",
    "    \n",
    "    return average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the actual model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function to predict the emoji from a sentence\n",
    "def predict(X, Y, W, b, word_to_vec_map):\n",
    "    '''\n",
    "    Aim: \n",
    "        Given X (sentences) and Y (emoji indices), predict emojis and compute\n",
    "        the accuracy of the model over the given set.\n",
    "        \n",
    "    Arguments:\n",
    "        X - input data containing sentences, numpy array of shape (m, None)\n",
    "        Y - labels, containing index of the label emoji, numpy array of shape (m,1)\n",
    "        \n",
    "    Returns:\n",
    "        pred - numpy array of shape (m,1) with your predictions\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m,1))\n",
    "    \n",
    "    for j in range(m): #loop over the training examples\n",
    "        #split the jth test example (sentence) into list of lower case words\n",
    "        words = X[j].lower()\n",
    "        \n",
    "        #average words' vectors\n",
    "        avg = sentence_to_avg(words, word_to_vec_map)\n",
    "        \n",
    "        #forward propogation\n",
    "        z = np.dot(W, avg) + b\n",
    "        a = softmax(z)\n",
    "        \n",
    "        pred[j] = np.argmax(a)\n",
    "        \n",
    "    print(f\"Accuracy: {np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))}\")\n",
    "    return pred\n",
    "\n",
    "def print_predictions(X, pred):\n",
    "    print()\n",
    "    for i in range(X.shape[0]):\n",
    "        print(X[i], label_to_emoji(int(pred[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate, num_iterations, print_every=100):\n",
    "    \n",
    "    #random seed\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    #number of training examples\n",
    "    m = Y.shape[0]\n",
    "    #number of classes\n",
    "    n_y = len(emoji_dictionary)\n",
    "    #dimensions of the glove vector\n",
    "    n_h = word_to_vec_map['the'].shape[0]\n",
    "    \n",
    "    #Initialize the parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h)/np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    #Convert Y to one hot encoded vector\n",
    "    Y_oh = convert_to_one_hot(Y, n_y)\n",
    "    \n",
    "    costs = []\n",
    "    #Optimization\n",
    "    for t in range(num_iterations): #loop over the number of iterations\n",
    "        for i in range(m): #loop over the training examples\n",
    "            \n",
    "            #get the sentence word2vec representation\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "            \n",
    "            #forward propogate the average through the softmax layer\n",
    "            #linear step\n",
    "            z = np.dot(W, avg) + b\n",
    "            #non-linear step\n",
    "            a = softmax(z)\n",
    "            \n",
    "            #compute the cross entropy loss \n",
    "            cost = -np.sum(np.multiply(Y_oh[i], np.log(a)))\n",
    "            #compute the gradients\n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1,n_h))\n",
    "            db = dz\n",
    "            \n",
    "            #update parameters with stochastic gradient descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        costs.append(cost)\n",
    "        if t % print_every == 0:\n",
    "            print(f\"Epoch {t} Cost: {cost}\")\n",
    "            pred = predict(X,Y,W,b,word_to_vec_map)\n",
    "            \n",
    "    plt.plot(range(num_iterations), costs)\n",
    "    plt.xlabel(\"Iterations\"); plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map, learning_rate=0.01, num_iterations=500, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random guessing would have had 20% accuracy given that there are 5 classes. This is pretty good performance after training on only 127 examples.\n",
    "\n",
    "In the training set, the algorithm saw the sentence \"I love you\" with the label ❤️. You can check however that the word \"adore\" does not appear in the training set. Nonetheless, lets see what happens if you write \"I adore you.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\",\n",
    "                          \"i miss elvis\", \"help save me!\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Because adore has a similar embedding as love, the algorithm has generalized correctly even to a word it has never seen before. Words such as heart, dear, beloved or adore have embedding vectors similar to love, and so might work too.\n",
    "\n",
    "Note though that it doesn't get \"not feeling happy\" correct. This algorithm ignores word ordering, so is not good at understanding phrases like \"not happy.\"\n",
    "\n",
    "### Confusion Matrix\n",
    "Printing the confusion matrix can also help understand which classes are more difficult for the model. A confusion matrix shows how often an example whose label is one class (\"actual\" class) is mislabeled by the algorithm with a different class (\"predicted\" class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for the confusion matrix\n",
    "import pandas as pd\n",
    "def print_predictions(X, pred):\n",
    "    print()\n",
    "    for i in range(X.shape[0]):\n",
    "        print(X[i], label_to_emoji(int(pred[i])))\n",
    "        \n",
    "        \n",
    "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "    \n",
    "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
    "    \n",
    "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y_test.shape)\n",
    "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "plot_confusion_matrix(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even with a 127 training examples, you can get a reasonably good model for Emojifying. This is due to the generalization power word vectors gives you.\n",
    "\n",
    "- This algorithm will perform poorly on sentences such as \"This movie is not good and not enjoyable\" because it doesn't understand combinations of words--it just averages all the words' embedding vectors together, without paying attention to the ordering of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model\n",
    "Let's build an LSTM model that takes as input word sequences. This model will be able to take word ordering into account.\n",
    "\n",
    "### Padding\n",
    "\n",
    "Most deep learning frameworks require that all sequences in the same mini-batch have the same length. This is what allows vectorization to work: If you had a 3-word sentence and a 4-word sentence, then the computations needed for them are different (one takes 3 steps of an LSTM, one takes 4 steps) so it's just not possible to do them both at the same time.\n",
    "\n",
    "The common solution to this is to use padding. Specifically, set a maximum sequence length, and pad all sequences to the same length. For example, of the maximum sequence length is 20, we could pad every sentence with \"0\"s so that each input sentence is of length 20. Thus, a sentence \"i love you\" would be represented as $(e_i, e_{love}, e_{you}, \\vec{0}, \\vec{0}, \\vec{0})$. In this example, any sentences longer than 20 words would have to be truncated. One simple way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer and Building the Vocabulary\n",
    "The embedding layer maps positive integers (indices corresponding to words) into dense vectors of fixed size (the embedding vectors). It can be trained or initialized with a pretrained embedding. Here we will create an embedding layer and initialize it with the GloVe 50-dimensional vectors loaded earlier in the notebook. Because our training set is quite small, we will not update the word embeddings but will instead leave their values fixed.\n",
    "\n",
    "The `Embedding()` layer takes an integer matrix of size `(batch size, max input length)` as input. This corresponds to sentences converted into lists of indices (integers), as shown in the figure below.\n",
    "\n",
    "The largest integer (i.e. word index) in the input should be no larger than the vocabulary size. The layer outputs an array of shape (batch size, max input length, dimension of word vectors).\n",
    "\n",
    "The first step is to convert all the training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the length of the longest sentence\n",
    "maxLen = len(max(X_train, key=len).split())\n",
    "print(f\"Length of the longest sentence: {maxLen} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    '''\n",
    "    Aim:\n",
    "        Convert an array of sentences into an array of indices corresponding to words in the sentences.\n",
    "        The output shape should be such that it can be fed to the embedding layer.\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0] #number of training examples\n",
    "    X_indices = np.zeros((m,max_len), dtype=int) #create an array of shape (batch_size, max_len)\n",
    "    for i in range(m):\n",
    "        sentence = [j.lower() for j in X[i].split()]\n",
    "        \n",
    "        k = 0\n",
    "        \n",
    "        for word in sentence:\n",
    "            X_indices[i,k] = word_to_index[word]\n",
    "            k+=1\n",
    "            \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create the word2vec weight matrix for the embedding layer. We're going to use the GloVe pretrained word embeddings for our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a zero matrix with rows = length of the vocabulary\n",
    "#and columns = word embedding dimension\n",
    "embedding_dimension = word_to_vec_map['the'].shape[0]\n",
    "weights_matrix = np.zeros((len(word_to_index)+1, embedding_dimension))\n",
    "\n",
    "words_not_found = {}\n",
    "\n",
    "for word,index in word_to_index.items():\n",
    "    try: \n",
    "        weights_matrix[index,:] = word_to_vec_map[word]\n",
    "    except KeyError:\n",
    "        weights_matrix[index,:] = np.random.normal(scale=0.6, size=(embedding_dimension, ))\n",
    "        words_not_found[word] = 1\n",
    "        \n",
    "print(f\"Number of words not in GloVe word embeddings map: {len(words_not_found)}\")\n",
    "print(f\"Shape of Weights Matrix: {weights_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the embedding layer. Since our vocabulary size is quite small, **and** and we dont have any words in the data which the embedding layer does not overlap with, we're going to make our word embeddings non trainable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "    weights_matrix = torch.from_numpy(weights_matrix)\n",
    "    num_embeddings, embedding_dim = weights_matrix.size()\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over all the different mappings we have so far:\n",
    "\n",
    "1. word_to_vec_map - map of all words in the GloVe word embeddings file with their corresponding vectors\n",
    "2. word_to_index - GloVe vocabulary words (str) mapped to a unique index (int)\n",
    "3. index_to_word - GloVe vocabulary word indices (int) mapped to the word (str)\n",
    "4. vocab - vocabulary of unique words in GloVe data\n",
    "5. weights_matrix - matrix of words in our vocabulary data mapped to their corresponding GloVe word embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting words in each sentence to indices from word_to_index\n",
    "train_X = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "test_X = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "# train_y = convert_to_one_hot(Y=Y_train,C=5)\n",
    "# test_y = convert_to_one_hot(Y=Y_test,C=5)\n",
    "print(f\"Shape of training set: {train_X.shape}\")\n",
    "print(f\"Shape of test set: {test_X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_X), torch.from_numpy(Y_train))\n",
    "test_data = TensorDataset(torch.from_numpy(test_X), torch.from_numpy(Y_test))\n",
    "\n",
    "batch_size = 33\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Train tensor size: {len(train_loader.dataset)}\")\n",
    "print(f\"Test tensor size: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class EmojiRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights_matrix, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(EmojiRNN, self).__init__()\n",
    "        \n",
    "        self.embedding, self.num_embeddings, self.embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = drop_prob\n",
    "        #LSTM layers\n",
    "        self.lstm1 = nn.LSTM(self.embedding_dim, self.hidden_dim, n_layers, \n",
    "                             dropout=self.dropout, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(self.hidden_dim, self.hidden_dim, n_layers,\n",
    "                            dropout=self.dropout, batch_first=True)\n",
    "        \n",
    "        #dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # linear and softmax layers\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_size)        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        print(x.size())\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm1_out, hidden1 = self.lstm1(embeds, hidden)\n",
    "        lstm2_out, hidden2 = self.lstm2(lstm1_out, hidden)\n",
    "        lstm2_out = lstm2_out.contiguous().view(-1, self.hidden_dim)\n",
    "        # fully-connected layer\n",
    "        out = self.fc(lstm2_out)\n",
    "        out = out.view(batch_size,-1,self.output_size)\n",
    "        # sigmoid function\n",
    "        softmax_out = F.softmax(out)\n",
    "#         print(softmax_out.size())\n",
    "        # reshape to (hidden_dim, batch_size, n_classes)\n",
    "        softmax_out = softmax_out.view(-1, batch_size, self.output_size)\n",
    "        softmax_out = softmax_out[0] # get last batch of labels\n",
    "        print(softmax_out.size())\n",
    "        # return last sigmoid output and hidden state\n",
    "        return softmax_out, hidden2\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(word_to_index)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size=5\n",
    "hidden_dim = 128\n",
    "n_layers = 32\n",
    "\n",
    "net = EmojiRNN(weights_matrix=weights_matrix, output_size=output_size, hidden_dim=hidden_dim, n_layers=n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 50 # 2 should be enough for this task\n",
    "\n",
    "print_every = 3\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    counter = 0\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = [w.lower() for w in X[i].split()]\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j += 1\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(5)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
